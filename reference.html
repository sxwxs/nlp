<!DOCTYPE html>
<html lang="zh_cn">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>附录3 参考文献</title>
        <link rel="stylesheet" href="https://es2q.com/nlp/theme/css/main.css" />
        <meta name="description" content="语料库和数据集" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://es2q.com/nlp/">自然语言处理实战-在线资源</a></h1>
                <nav><ul>
                    <li class="active"><a href="https://es2q.com/nlp/category/nlp.html">NLP</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://es2q.com/nlp/reference.html" rel="bookmark"
           title="Permalink to 附录3 参考文献">附录3  参考文献</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2022-04-26T22:00:00+08:00">
                Published: 周二 26 四月 2022
        </abbr>
		<br />
        <abbr class="modified" title="2022-04-26T22:30:00+08:00">
                Updated: 周二 26 四月 2022
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://es2q.com/nlp/author/sxw.html">sxw</a>
        </address>
<p>In <a href="https://es2q.com/nlp/category/nlp.html">NLP</a>.</p>
<p>tags: <a href="https://es2q.com/nlp/tag/nlp.html">NLP</a> </p>
</footer><!-- /.post-info -->      <h3>1.1.4 自然语言处理的发展历程</h3>
<ol>
<li>文章“Giving GPT-3 a Turing Test”</li>
</ol>
<p>https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html</p>
<h3>1.3.6 注意力机制</h3>
<ol>
<li>开源项目：文字注意力热力图可视化（Text-Attention-Heatmap-Visualization）</li>
</ol>
<p>https://github.com/jiesutd/Text-Attention-Heatmap-Visualization</p>
<h3>1.3.8 多模态学习</h3>
<ol>
<li>论文“Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books”</li>
</ol>
<p>https://arxiv.org/abs/1506.06724</p>
<h3>1.4.2  Batch Size的选择</h3>
<ol>
<li>论文“Revisiting Small Batch Training for Deep Neural Networks”</li>
</ol>
<p>https://arxiv.org/abs/1804.07612</p>
<h3>1.4.3  数据集不平衡问题</h3>
<ol>
<li>论文“Focal Loss for Dense Object Detection”</li>
</ol>
<p>https://arxiv.org/abs/1708.02002</p>
<ol>
<li>论文“A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection” </li>
</ol>
<p>https://arxiv.org/abs/1607.07155</p>
<h3>1.5.4  预训练模型与数据安全</h3>
<ol>
<li>论文“Extracting Training Data from Large Language Models”</li>
</ol>
<p>https://arxiv.org/abs/2012.07805</p>
<h3>2.1.3  使用pip包管理程序和Python虚环境</h3>
<ol>
<li>Python官方文档：虚环境</li>
</ol>
<p>https://docs.python.org/zh-cn/3/tutorial/venv.html</p>
<h3>2.1.5  安装Python自然语言处理常用的库</h3>
<ol>
<li>论文“PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation”</li>
</ol>
<p>https://arxiv.org/abs/1906.11455</p>
<h3>2.3.5 文本规范化</h3>
<ol>
<li>BERT-KPE</li>
</ol>
<p>https://github.com/thunlp/BERT-KPE/blob/master/preprocess/prepro_utils.py</p>
<h3>2.5.1 通过ctype调用C/C++代码</h3>
<ol>
<li>Python官方文档ctypes</li>
</ol>
<p>https://docs.python.org/zh-cn/3.8/library/ctypes.html</p>
<h3>3.2.1 PyTorch的优势</h3>
<ol>
<li>近年来几个NLP顶级会议中使用PyTorch和TensorFlow论文数比较数据</li>
</ol>
<p>http://horace.io/pytorch-vs-tensorflow/</p>
<h3>4.2.6 使用torch.nn的Transformer模型</h3>
<ol>
<li>论文“Attention is all you need”</li>
</ol>
<p>https://arxiv.org/abs/1706.03762</p>
<h3>4.3.6 使用LogSoftMax函数</h3>
<ol>
<li>PyTorch中SoftMax和LogSoftMax的实现（C++代码）地址</li>
</ol>
<p>https://github.com/pytorch/pytorch/blob/v1.6.0/aten/src/ATen/native/SoftMax.cpp</p>
<h3>4.5.2 使用Adam优化器</h3>
<ol>
<li>论文“Adam: A Method for Stochastic Optimization”</li>
</ol>
<p>https://arxiv.org/abs/1412.6980</p>
<ol>
<li>论文“On the Convergence of Adam and Beyond”</li>
</ol>
<p>https://arxiv.org/abs/1904.09237</p>
<h3>4.5.3 使用AdamW优化器</h3>
<ol>
<li>论文“Decoupled Weight Decay Regularization”</li>
</ol>
<p>https://arxiv.org/abs/1711.05101</p>
<h3>4.9.2 在PyTorch中使用TensorBoard</h3>
<p>PyTorch官方网站文档中对TensorBoard使用方法的介绍</p>
<p>https://pytorch.org/docs/master/tensorboard.html#torch-utils-tensorboard</p>
<h3>6.3.4 使用pkuseg</h3>
<ol>
<li>开源项目pkuseg-python中提供的tags.txt文件</li>
</ol>
<p>https://github.com/lancopku/pkuseg-python/blob/master/tags.txt</p>
<h3>9.1.1 背景</h3>
<ol>
<li>论文“Generating Sequences With Recurrent Neural Networks”</li>
</ol>
<p>https://arxiv.org/abs/1308.0850</p>
<p>论文“Recurrent Continuous Translation Models”</p>
<p>https://www.aclweb.org/anthology/D13-1176/</p>
<p>论文“Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation”</p>
<p>https://arxiv.org/abs/1406.1078</p>
<p>论文“Sequence to Sequence Learning with Neural Networks”</p>
<p>https://arxiv.org/abs/1409.3215</p>
<h3>9.2 使用PyTorch实现Seq2seq模型</h3>
<ol>
<li>开源项目PyTorch-Seq2seq</li>
</ol>
<p>https://github.com/bentrevett/pytorch-seq2seq</p>
<h3>10.1.1  最早应用于计算机视觉</h3>
<ol>
<li>文章“Attention and Memory in Deep Learning and NLP”</li>
</ol>
<p>http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/</p>
<ol>
<li>论文“Recurrent Models of Visual Attention”</li>
</ol>
<p>http://arxiv.org/abs/1406.6247</p>
<h3>10.4.2  Self-Attention相关的工作</h3>
<ol>
<li>论文“Long Short-Term Memory-Networks for Machine Reading”</li>
</ol>
<p>https://www.aclweb.org/anthology/D16-1053/</p>
<p>论文“A Structured Self-Attentive Sentence Embedding”</p>
<p>https://arxiv.org/abs/1703.03130</p>
<p>论文“A Deep Reinforced Model for Abstractive Summarization”</p>
<p>https://arxiv.org/abs/1705.04304</p>
<h3>10.6 Multi-hop Attention</h3>
<ol>
<li>论文“Memory Networks”</li>
</ol>
<p>https://arxiv.org/abs/1410.3916</p>
<p>论文“End-To-End Memory Networks”</p>
<p>https://arxiv.org/abs/1503.08895</p>
<p>论文“Multihop Attention Networks for Question Answer Matching”</p>
<p>https://dl.acm.org/doi/10.1145/3209978.3210009</p>
<h3>10.7 Soft Attention和Hard Attention</h3>
<p>论文“Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”</p>
<p>https://arxiv.org/abs/1502.03044</p>
<h3>10.8 Full Attention和Sparse Attention</h3>
<p>论文“Generating Long Sequences with Sparse Transformers”</p>
<p>https://arxiv.org/abs/1904.10509</p>
<h3>11.1.  背景</h3>
<ol>
<li>论文“Attention Is All You Need”</li>
</ol>
<p>https://arxiv.org/abs/1706.03762</p>
<p>论文“Convolutional Sequence to Sequence Learning”</p>
<p>https://arxiv.org/abs/1705.03122</p>
<h3>11.2.1  背景</h3>
<ol>
<li>论文“Convolutional Neural Networks for Sentence Classification”</li>
</ol>
<p>https://arxiv.org/abs/1408.5882</p>
<h3>11.3.4  使用Positional Encoding</h3>
<ol>
<li>Positional Encoding完整代码</li>
</ol>
<p>https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipyn</p>
<h3>11.4 Transformer的改进</h3>
<ol>
<li>论文“Generating Long Sequences with Sparse Transformers”</li>
</ol>
<p>https://arxiv.org/abs/1904.10509</p>
<p>论文“Local Self-Attention over Long Text for Efficient Document Retrieval”</p>
<p>https://arxiv.org/abs/2005.04908</p>
<h3>12.1.3 自然语言处理预训练的发展</h3>
<ol>
<li>论文“Deep Contextualized Word Representations”</li>
</ol>
<p>https://arxiv.org/abs/1802.05365</p>
<h3>12.3 GPT模型</h3>
<ol>
<li>论文“Improving Language Understanding by Generative Pre-Training”</li>
</ol>
<p>https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</p>
<h3>12.3.6 GPT2和GPT3</h3>
<ol>
<li>论文“Language Models are Unsupervised Multitask Learners”</li>
</ol>
<p>https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf</p>
<p>论文“Language Models are Few-Shot Learners”</p>
<p>https://arxiv.org/abs/2005.14165</p>
<p>文章“Giving GPT-3 a Turing Test”</p>
<p>https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html</p>
<h3>12.4 BERT模型</h3>
<ol>
<li>论文“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”</li>
</ol>
<p>https://arxiv.org/abs/1810.04805</p>
<p>论文“Cloze procedure: A new tool for measuring readability”</p>
<p>https://journals.sagepub.com/doi/10.1177/107769905303000401</p>
<p>论文“RoBERTa: A Robustly Optimized BERT Pretraining Approach”</p>
<p>https://arxiv.org/abs/1907.11692</p>
<p>论文“ALBERT: A Lite BERT for Self-supervised Learning of Language Representations”</p>
<p>https://arxiv.org/abs/1909.11942</p>
<h3>14.1.1  实验目标与数据集介绍</h3>
<ol>
<li>论文“"Neural Chinese Address Parsing”</li>
</ol>
<p>https://www.aclweb.org/anthology/N19-1346/</p>
<h3>14.4.4 训练模型</h3>
<ol>
<li>neural-chinese-address-parsing 中包含的测试脚本conlleval.pl</li>
</ol>
<p>https://github.com/leodotnet/neural-chinese-address-parsing/blob/master/conlleval.pl</p>
<h3>15.7.2  评估模型</h3>
<ol>
<li>GPT2-Chinese</li>
</ol>
<p>https://github.com/Morizeyao/GPT2-Chinese/blob/master/generate.py</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://es2q.com/blog/">我的博客</a></li>
                            <li><a href="https://github.com/sxwxs">我的GitHub</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>